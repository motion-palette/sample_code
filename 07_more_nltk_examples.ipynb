{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK — Additional Examples\n",
    "\n",
    "http://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Text Corpora and Lexical Resources\n",
    "\n",
    "NLTK has many books/texts available, including the Gutenberg Corpus, Web and Chat Text, Brown Corpus, Reuters Corpus, Inaugural Address Corpus, various annotated corpora (e.g,. WordNet), as well as corpora in non-English languages.\n",
    "\n",
    "The following is a simple example of how you can use a corpus (in this case the **Gutenberg Corpus**).\n",
    "\n",
    "If you are interested in other corpora, look at the following page for example code.\n",
    "\n",
    "> http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
      "\n",
      "Displaying 10 of 10 matches:\n",
      "xceedingly ; twelve princes shall he beget , and I will make him a great nation\n",
      "jealous God . 4 : 25 When thou shalt beget children , and children ' s children\n",
      " cast his fruit . 28 : 41 Thou shalt beget sons and daughters , but thou shalt \n",
      "l issue from thee , which thou shalt beget , shall they take away ; and they sh\n",
      " is an evil disease . 6 : 3 If a man beget an hundred children , and live many \n",
      "l issue from thee , which thou shalt beget , shall they take away ; and they sh\n",
      "of them ; 29 : 6 Take ye wives , and beget sons and daughters ; and take wives \n",
      ", saith the Lord GOD . 18 : 10 If he beget a son that is a robber , a shedder o\n",
      " upon him . 18 : 14 Now , lo , if he beget a son , that seeth all his father ' \n",
      "that sojourn among you , which shall beget children among you : and they shall \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "tokens = gutenberg.words('bible-kjv.txt')   # get the words from the King James Bible. It returns a list of tokens.\n",
    "#print(kjv[0:50])                           \n",
    "kjv = nltk.Text(kjv)                        # Convert the list of tokens into an nltk.Text object.\n",
    "print(\"{}\\n\".format(kjv[25:36]))            # print the characters between indexes 25 and 36.\n",
    "kjv.concordance(\"beget\")                    # create concordances with 'beget'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                              # load the NLTK package\n",
    "from nltk.corpus import gutenberg        # import 'gutenberg' (corpus) from the nltk.corpora module\n",
    "gutenberg.fileids()                      # list the text files included in the Gutenberg corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into an nltk.Text object\n",
    "\n",
    "If you convert your own text (from a file) into an nltk.Text object, you can do a few things more easily. One of the most useful tools is the concordance tool. The following is an example of how you can use the concordance tool on your text. \n",
    "\n",
    "I have also included a few more useful tools below in separate code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 76 matches:\n",
      "me . thank you for bringing marc into our family , and charlotte and aidan into\n",
      "ight . and to those of you who joined our campaign this week , thank you . what\n",
      "use of his friendship . we heard from our terrific vice president , the one-and\n",
      "n . he spoke from his big heart about our party 's commitment to working people\n",
      " lady michelle obama reminded us that our children are watching , and the presi\n",
      "e 'll make the whole country proud as our vice president . and i want to thank \n",
      "who threw their hearts and souls into our primary . you 've put economic and so\n",
      "now , i 've heard you . your cause is our cause . our country needs your ideas \n",
      "heard you . your cause is our cause . our country needs your ideas , energy , a\n",
      "on . that is the only way we can turn our progressive platform into real change\n",
      "e to philadelphia – the birthplace of our nation – because what happened in thi\n",
      "hat took courage . they had courage . our founders embraced the enduring truth \n",
      "espect are fraying . and just as with our founders , there are no guarantees . \n",
      "ogether so we all can rise together . our country 's motto is e pluribus unum :\n",
      "lf.” now we are clear-eyed about what our country is up against . but we are no\n",
      "rants who are already contributing to our economy ! we will not ban a religion \n",
      ". we will work with all americans and our allies to fight and defeat terrorism \n",
      " . so do n't let anyone tell you that our country is weak . we 're not . do n't\n",
      " together.” and remember . remember : our founders fought a revolution and wrot\n",
      " and forty years later , we still put our faith in each other . look at what ha\n",
      "america needs every one of us to lend our energy , our talents , our ambition t\n",
      " every one of us to lend our energy , our talents , our ambition to making our \n",
      "us to lend our energy , our talents , our ambition to making our nation better \n",
      " our talents , our ambition to making our nation better and stronger . i believ\n",
      "r together” is not just a lesson from our history . it 's not just a slogan for\n",
      "--\n",
      "donald trump; years ago; police officers; united states; small\n",
      "businesses; 're going; young people; could n't; years later; middle\n",
      "class; sales pitch; tax breaks; tim kaine; health care; vice\n",
      "president; bernie sanders; lauren manning; tuesday night; clean\n",
      "energy; fix it.”\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize   \n",
    "\n",
    "fin = open(\"text/clinton_dnc_speech_2016.txt\")   # open a file\n",
    "tokens = word_tokenize(fin.read())               # tokenize the text in the file\n",
    "words = [w.lower() for w in tokens]              # 'lowercase' all the words\n",
    "text = nltk.Text(words)                          # create an nltk.Text object\n",
    "text.concordance('our')                          # get concordances for 'our'\n",
    "print(\"--\")\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifetimes talents cause\n",
      "None\n",
      "---\n",
      "thank\n",
      "---\n",
      "76\n",
      "---\n",
      "1378\n"
     ]
    }
   ],
   "source": [
    "# More nltk.Text functions\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize   \n",
    "fin = open(\"text/clinton_dnc_speech_2016.txt\")   # open a file\n",
    "fin.seek(0)\n",
    "tokens = word_tokenize(fin.read())               # tokenize the text in the file\n",
    "words = [w.lower() for w in tokens]              # 'lowercase' all the words\n",
    "text = nltk.Text(words)                          # create an nltk.Text object\n",
    "\n",
    "print(text.similar('energy'))   # find all words that share a common context\n",
    "print(\"---\")\n",
    "print(text[3])                  # 5th token\n",
    "print(\"---\")\n",
    "print(text.count('our'))\n",
    "print(\"---\")\n",
    "print(len(set(text)))           # set(text) returns a list of only one of each word (types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more nltk.Text functions...\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize   \n",
    "fin = open(\"text/clinton_dnc_speech_2016.txt\")   # open a file\n",
    "fin.seek(0)\n",
    "tokens = word_tokenize(fin.read())               # tokenize the text in the file\n",
    "words = [w.lower() for w in tokens]              # 'lowercase' all the words\n",
    "text = nltk.Text(words)                          # create an nltk.Text object\n",
    "\n",
    "res = [w for w in text if w.isalpha()]           # keep alphabetic characters\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "0.03349282296650718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more nltk.Text functions...\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize   \n",
    "fin = open(\"text/clinton_dnc_speech_2016.txt\")   # open a file\n",
    "fin.seek(0)\n",
    "tokens = word_tokenize(fin.read())               # tokenize the text in the file\n",
    "words = [w.lower() for w in tokens]              # 'lowercase' all the words\n",
    "text = nltk.Text(words)                          # create an nltk.Text object\n",
    "\n",
    "fd = nltk.FreqDist(text)             # create a new data object that contains information about word frequency\n",
    "print( fd['our'] ) \n",
    "#print( fd.keys() )                   # a list of unique words (types)\n",
    "#print( fd.items() )                  # a list of everything in the text (including punctuations, repeated words etc.)\n",
    "#fd.plot(10, cumulative=False)        # generate a chart of the 10 most frequent words\n",
    "print(fd.freq('the'))\n",
    "res =([len(w) for w in text])          # list of word length\n",
    "#print(res)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('thank', 'you'),\n",
      "    ('you', '!'),\n",
      "    ('!', 'thank'),\n",
      "    ('thank', 'you'),\n",
      "    ('you', 'all'),\n",
      "    ('all', 'very'),\n",
      "    ('very', 'much'),\n",
      "    ('much', '!'),\n",
      "    ('!', 'thank'),\n",
      "    ('thank', 'you'),\n",
      "    ('you', 'for'),\n",
      "    ('for', 'that')]\n",
      "\n",
      "[   ('thank', 'you', '!'),\n",
      "    ('you', '!', 'thank'),\n",
      "    ('!', 'thank', 'you'),\n",
      "    ('thank', 'you', 'all'),\n",
      "    ('you', 'all', 'very'),\n",
      "    ('all', 'very', 'much'),\n",
      "    ('very', 'much', '!'),\n",
      "    ('much', '!', 'thank'),\n",
      "    ('!', 'thank', 'you'),\n",
      "    ('thank', 'you', 'for'),\n",
      "    ('you', 'for', 'that'),\n",
      "    ('for', 'that', 'amazing')]\n",
      "\n",
      "[   ('thank', 'you', '!', 'thank', 'you'),\n",
      "    ('you', '!', 'thank', 'you', 'all'),\n",
      "    ('!', 'thank', 'you', 'all', 'very'),\n",
      "    ('thank', 'you', 'all', 'very', 'much'),\n",
      "    ('you', 'all', 'very', 'much', '!'),\n",
      "    ('all', 'very', 'much', '!', 'thank'),\n",
      "    ('very', 'much', '!', 'thank', 'you'),\n",
      "    ('much', '!', 'thank', 'you', 'for'),\n",
      "    ('!', 'thank', 'you', 'for', 'that'),\n",
      "    ('thank', 'you', 'for', 'that', 'amazing'),\n",
      "    ('you', 'for', 'that', 'amazing', 'welcome'),\n",
      "    ('for', 'that', 'amazing', 'welcome', '.')]\n"
     ]
    }
   ],
   "source": [
    "import pprint                                  # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object for debugging\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize   \n",
    "fin = open(\"text/clinton_dnc_speech_2016.txt\")   # open a file\n",
    "fin.seek(0)\n",
    "tokens = word_tokenize(fin.read())               # tokenize the text in the file\n",
    "words = [w.lower() for w in tokens]              # 'lowercase' all the words\n",
    "text = nltk.Text(words)                          # create an nltk.Text object\n",
    "\n",
    "res = nltk.bigrams(text)\n",
    "pp.pprint(list(res)[:12])\n",
    "print(\"\")\n",
    "res = nltk.trigrams(text)\n",
    "pp.pprint(list(res)[:12])\n",
    "print(\"\")\n",
    "res = nltk.ngrams(text, 5)\n",
    "pp.pprint(list(res)[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
