{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python for Everybody\n",
    "## Chapter 12. Networked Programs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 12.1 HyperText Transport Protocol - HTTP\n",
    "# Read the textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can skip the coding samples in the following sections\n",
    "- 12.2 The World's Simplest Web Browser\n",
    "- 12.3 Retrieving an Image over HTTP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'sun': 2, b'fair': 1, b'and': 3, b'But': 1, b'grief': 1, b'east': 1, b'light': 1, b'is': 3, b'pale': 1, b'window': 1, b'sick': 1, b'soft': 1, b'what': 1, b'through': 1, b'already': 1, b'the': 3, b'Arise': 1, b'Juliet': 1, b'with': 1, b'It': 1, b'yonder': 1, b'breaks': 1, b'Who': 1, b'envious': 1, b'kill': 1, b'moon': 1}\n"
     ]
    }
   ],
   "source": [
    "# 12.4 Retrieving web pages with urllib\n",
    "\n",
    "import urllib.request                 # import the urllib.request module\n",
    "                                      # Using this module, you can'talk' to web servers.\n",
    "counts = dict()                       \n",
    "\n",
    "# utlopen() is like open, except the file is on a remote server.\n",
    "fhand = urllib.request.urlopen('http://www.py4inf.com/code/romeo.txt')   \n",
    "\n",
    "for line in fhand:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cmu.edu/news/feeds/news.rss\n",
      "http://www.cmu.edu/\n",
      "http://www.library.cmu.edu/\n",
      "http://admission.enrollment.cmu.edu/\n",
      "http://www.cmu.edu/graduate/admissions/\n",
      "http://www.cmu.edu/leadership/\n",
      "http://athletics.cmu.edu\n",
      "http://www.giving.cmu.edu/s/1410/giving/16/landing.aspx?sid=1410&amp;gid=1&amp;pgid=7010\n",
      "http://www.cmu.edu/news/stories/archives/2016/october/historymakers.html\n",
      "http://cmtoday.cmu.edu/artsculture_entertainment/artistic-double-play/\n",
      "http://cmtoday.cmu.edu/publicpolicy_innovation/improving-global-health-one-shot-at-a-time/\n",
      "http://www.cmu.edu/news/stories/archives/2016/october/lackner-wins-200th%20.html\n",
      "http://www.cmu.edu/news/stories/archives/2016/october/frontiers-post.html\n",
      "http://www.cmu.edu/news/stories/archives/2016/october/colbert-obama.html\n",
      "http://www.cmu.edu/news\n",
      "http://www.cmu.edu/leadership/pres-fellow-scholar/\n",
      "http://www.cmu.edu/brain\n",
      "http://www.cmu.edu/energy\n",
      "http://www.cmu.edu/simon\n",
      "http://www.cmu.edu/strategic-plan/index.html\n",
      "http://www.cmu.edu/news/stories/archives/2016/august/jose-oubrerie.html\n",
      "http://www.cmu.edu/dietrich/news/news-stories/2016/october/gates-giler-lecture.html\n",
      "http://www.cmu.edu/swartz-center-for-entrepreneurship/events/launchcmu/\n",
      "http://music.cmu.edu/events/590\n",
      "http://www.cmu.edu/hss/globalstudies/events/Immigration,%20Identity,%20and%20the%20Transformation%20of%20Pittsburgh.html\n",
      "http://www.cmu.edu/student-affairs/dean/multicultural/dialogue/racism-is-real.html\n",
      "http://www.cmu.edu/ir/cirp-policy-forum/fall%202016/october-28-gen-ret-michael-hayden.html\n",
      "http://www.cmu.edu/events/\n",
      "http://www.cmu.edu/jobs/\n",
      "http://www.cmu.edu/global/presence/\n",
      "http://www.cmu.edu/news/\n",
      "http://www.cmu.edu/title-ix/\n",
      "http://www.alumni.cmu.edu/s/1410/alumni/start.aspx\n",
      "http://www.cmu.edu/legal/\n",
      "http://www.cmu.edu/\n",
      "http://www.facebook.com/carnegiemellonu\n",
      "http://www.twitter.com/carnegiemellon\n",
      "http://www.linkedin.com/groupInvitation?groupID=2221&amp;sharedKey=7BB90F3D4115\n",
      "http://www.youtube.com/carnegiemellonu\n",
      "http://itunes.apple.com/us/institution/carnegie-mellon-university/id383908951\n",
      "http://weibo.com/cmualumni\n"
     ]
    }
   ],
   "source": [
    "# 12.5 & 12.5 Parsing HTML and Scraping the Web Using Regular Expressions\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://www.cmu.edu\"\n",
    "fhand = urllib.request.urlopen(url)        # open a URL\n",
    "# print(type(fhand))                       # fhand is a 'http.client.HTTPResponse' object\n",
    "html  = fhand.read()\n",
    "# print(type(html))                        # http.client.HTTPResponse.read() returns a 'bytes' objct\n",
    "htmlStr = html.decode('utf-8', 'ignore')   # decode the bytes object into a UNICODE string, and\n",
    "                                           # 'ignore' any erros.\n",
    "\n",
    "links = re.findall('href=\"(http://.*?)\"', htmlStr)    # find all the URLs (links) on this page\n",
    "for link in links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Install BeautifulSoup by executing the following command in your <u>virtual environment</u>:\n",
    "\n",
    "pip install bs4\n",
    "\n",
    "***\n",
    "\n",
    "### Commonly used BeautifulSoup methods\n",
    "\n",
    "#### soup methods\n",
    "```\n",
    "soup.find(id=\"xxxx\")         # find a tag with a specific id. (There is only 1 tag with a specific ID in HTML.)\n",
    "soup('a')                    # list all the 'a' tags in the page.\n",
    "soup.find_all(\"div\", { \"class\" : \"topstories\" })   # find all instances of a specific tag (e.g., <div>)\n",
    "                                                   # with a specific class (e.g., \"topstories\")\n",
    "```\n",
    "\n",
    "#### tag methods\n",
    "```\n",
    "tag.get('href', None)        # get the tag's attribute (e.g., 'href'. If there is no attribute with the \n",
    "                             # specified name, return None.\n",
    "tag.text                     # returns the tag's text content\n",
    "```                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "TAG:<a href=\"com.htm\"><img border=\"0\" src=\"icon2a.gif\">Come in!</img></a>\n",
      "URL:com.htm\n",
      "Content:<img border=\"0\" src=\"icon2a.gif\">Come in!</img>\n",
      "Attrs:{'href': 'com.htm'}\n",
      "Text:Come in!\n",
      "---\n",
      "TAG:<a href=\"history.htm\"><img border=\"0\" src=\"icon5a.gif\">Our history page</img></a>\n",
      "URL:history.htm\n",
      "Content:<img border=\"0\" src=\"icon5a.gif\">Our history page</img>\n",
      "Attrs:{'href': 'history.htm'}\n",
      "Text:Our history page\n",
      "---\n",
      "TAG:<a href=\"what.htm\"><img border=\"0\" src=\"icon4a.gif\"> Whats` New</img></a>\n",
      "URL:what.htm\n",
      "Content:<img border=\"0\" src=\"icon4a.gif\"> Whats` New</img>\n",
      "Attrs:{'href': 'what.htm'}\n",
      "Text: Whats` New\n",
      "---\n",
      "TAG:<a href=\"yuv_home.htm\"> y.</a>\n",
      "URL:yuv_home.htm\n",
      "Content: y.\n",
      "Attrs:{'href': 'yuv_home.htm'}\n",
      "Text: y.\n",
      "---\n",
      "TAG:<a href=\"guest.htm\">guest book</a>\n",
      "URL:guest.htm\n",
      "Content:guest book\n",
      "Attrs:{'href': 'guest.htm'}\n",
      "Text:guest book\n",
      "---\n",
      "TAG:<a href=\"mailto:mossad@eindor.org.il\">mossad@eindor.org.il</a>\n",
      "URL:mailto:mossad@eindor.org.il\n",
      "Content:mossad@eindor.org.il\n",
      "Attrs:{'href': 'mailto:mossad@eindor.org.il'}\n",
      "Text:mossad@eindor.org.il\n",
      "---\n",
      "TAG:<a href=\"http://www.oocities.com/\">Return to <i>GeoCities</i>.</a>\n",
      "URL:http://www.oocities.com/\n",
      "Content:Return to \n",
      "Attrs:{'href': 'http://www.oocities.com/'}\n",
      "Text:Return to GeoCities.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = \"http://www.oocities.org/rainforest/1035/\"\n",
    "html = urllib.request.urlopen(url).read()\n",
    "\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    ## Look at the parts of a tag\n",
    "    print(\"---\")\n",
    "    print('TAG:'+ str(tag))\n",
    "    print('URL:' + str(tag.get('href', None)))\n",
    "    print('Content:' + str(tag.contents[0]))\n",
    "    print('Attrs:' + str(tag.attrs))\n",
    "    print('Text:' + str(tag.text))\n",
    "    \n",
    "## Note: Unlike in the book, the 'tag' types in BeautifulSoup cannot be implicitly converted to string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The following program visits the Post Gazette's Opinion page (http://www.post-gazette.com/opinion) and finds a list of URLs for the current top stories. Then, the program visits each of the top stories, retrieves the story, and saves it as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://www.post-gazette.com/opinion/editorials/2016/10/24/China-gears-up-As-plenary-session-opens-internal-issues-rule/stories/201610220039\n",
      "\n",
      "URL: http://www.post-gazette.com/opinion/editorials/2016/10/24/Euro-Mars-angst-But-the-lander-s-flub-was-just-a-bump-on-its-mission/stories/201610240023\n",
      "\n",
      "URL: http://www.post-gazette.com/opinion/editorials/2016/10/24/Walk-on-the-mild-side-Stressed-Angry-Breathe-take-a-mellow-stroll/stories/201610310050\n",
      "\n",
      "URL: http://www.post-gazette.com/opinion/editorials/2016/10/23/Harrisburg-fiddles-Lawmakers-are-busy-avoiding-the-big-topics/stories/201610310046\n",
      "\n",
      "URL: http://www.post-gazette.com/opinion/editorials/2016/10/23/Ukraine-cont-Let-the-Europeans-hash-out-the-mess-with-Russia/stories/201610220034\n",
      "\n",
      "URL: http://blogs.post-gazette.com/opinion/rob-rogers-cartoons/47689-worthless-id\n",
      "\n",
      "URL: http://blogs.post-gazette.com/opinion/rob-rogers-cartoons/47676-transfer-of-power\n",
      "\n",
      "URL: http://blogs.post-gazette.com/opinion/rob-rogers-cartoons/47659-supreme-election\n",
      "\n",
      "URL: http://blogs.post-gazette.com/opinion/rob-rogers-cartoons/47642-rigged-election\n",
      "\n",
      "URL: http://blogs.post-gazette.com/opinion/rob-rogers-cartoons/47628-brewed-on-grant-ppg-arena\n",
      "\n",
      "URL: http://blogs.post-gazette.com/index.php/opinion/rob-rogers-cartoons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# ------------------------------------------------------------\n",
    "# Part I - Create a list of URLs for the current top stories in \n",
    "# the Opinions Section\n",
    "# ------------------------------------------------------------\n",
    "url = \"http://www.post-gazette.com/opinion\"    # URL\n",
    "fhand = urllib.request.urlopen(url)            # open the URL\n",
    "html = fhand.read()                            # read the page\n",
    "soup = bs(html, \"html.parser\")                        # create a beautifulsoup object\n",
    "\n",
    "topStories = soup.find_all(\"div\", { \"class\" : \"topstories\" })  # find all the <div> tags, where class = top stories.\n",
    "\n",
    "# Find all of the anchor ('a') tags in the <div>, and put it in the 'links' list.\n",
    "links = list()                           # Initialize 'links' with an empty list.\n",
    "tags = topStories[0].findAll('a')        # Find all the 'a' tags within the top stories <div>\n",
    "for tag in tags:                         # For each 'a' tag in a list of 'a' tags\n",
    "    c = str(tag.get('class', None))      # Get the value of the class attribute. Return None if there's no class attr.\n",
    "                                         # We know that if it is not None, the 'a' tag is not a story.\n",
    "    if c == \"None\":                      # if None (i.e., one of the top story linnks)\n",
    "        url = tag.get('href', None)      # get the URL attribute in the 'a' tag.\n",
    "        if url != None and url.startswith(\"http://\"):   # if url is not None, and starts with \"http://\"\n",
    "            links.append(url)\n",
    "fhand.close()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part II - Visit each top story page, retrieve the content, and \n",
    "# save its title and paragraphs (story) as a text file.\n",
    "# ------------------------------------------------------------\n",
    "for url in links:                            # for each URL in the list of URLs (top stories),\n",
    "    print(\"URL: {}\\n\".format(url))           # print the URL (just check to make sure it looks right)\n",
    "    fhand = urllib.request.urlopen(url)      # open the URL\n",
    "    html = fhand.read()                      # read the page\n",
    "    soup = bs(html, \"lxml\")                  # create a beautifulsoup object\n",
    "    story = soup.find(id=\"story\")            # find an element (tag) with id=\"story\"\n",
    "    \n",
    "    if story != None:                        # if 'story' is not None\n",
    "        header = story.findNext(\"h1\")        # find the <h1> tag.\n",
    "        body   = story.findNext(\"div\", { \"class\" : \"thisStory\" })            # find <div> with class=\"thisStory\"\n",
    "\n",
    "        if header != None and body != None:  # if header and body are both valid (!= None)\n",
    "            filename = \"text/{}.txt\".format(header.text.replace(\" \", \"-\"))   # construct a filename\n",
    "            fout = open(filename, \"w\")                  # open a new file to write            \n",
    "            fout.write(\"{}\\n\\n\".format(header.text))    # write the article's title\n",
    "\n",
    "            paragraphs = body.findAll('p')              # find all the <p> tags (i.e., paragraphs)\n",
    "            for p in paragraphs:                        # print each paragraph to the file.\n",
    "                fout.write(\"{}\\n\\n\".format(p.text))\n",
    "            fout.close()\n",
    "    fhand.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
