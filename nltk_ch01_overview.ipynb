{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing with NLTK3 Cookbook\n",
    "\n",
    "## Chapter 1: Tokenizing Text and WordNet Basics\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start executing the programs on this page...\n",
    "\n",
    "You need to install the Natural Language Tool Kit (NLTK). You can install the NLTK relatively quickly, but may take longer to download the NTLK corpora (10-15 minutes).\n",
    "\n",
    "### Installing the NLTK \n",
    "\n",
    "1. Open Terminal, and change your current folder to the course folder (e.g., ```Desktop/cfh/```)\n",
    "2. Activate the virtual environment\n",
    "3. Type the following line, and then press the return key.\n",
    "\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "You should now be successfully downloading the NLTK. (If something goes wrong here, please contact me).\n",
    "\n",
    "Once you have successfully downloaded the NLTK, you can download some NLTK corpora.\n",
    "\n",
    "### Downloading NLTK Corpora\n",
    "\n",
    "1. Open Terminal and change your current folder to the course folder (e.g., ```Desktop/cfh/```)\n",
    "2. Activate the virtual environment\n",
    "3. Start Python by typing 'python' in the command line\n",
    "4. Import the NLTK by typing: **```import nltk```**\n",
    "5. Launch the NLTK download app by typing: **```nltk.download()```**\n",
    "\n",
    "The following example shows Steps 3-5:\n",
    "\n",
    ">```\n",
    "(ven) <your_username>/Desktop/cfh % python\n",
    "Python 3.5.1 |Anaconda 4.0.0 (x86_64)| (default, Dec  7 2015, 11:24:55) \n",
    "[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>>> import nltk\n",
    ">>>> nltk.download()\n",
    "```\n",
    "\n",
    "Select the **Corpora** tab, and download the following items:\n",
    "\n",
    "- webtext\n",
    "- wordnet\n",
    "- stopwords\n",
    "\n",
    "**CAUTION !!** \u001f-â€” Do not double click these items. If you do, you will start downloading them. In particular, DO NOT double click \"all\", \"all-corpus\", or \"book\" under the Collections tab. You will start downloading MANY things, and it will take a LONG time.\n",
    "\n",
    "Reference: http://www.nltk.org/data.html\n",
    "\n",
    "***\n",
    "\n",
    "If you decide to download everything, do it when you do not need to use your computer for a while. It will take a lot of time.\n",
    "\n",
    "If you are interested in the complete listing of corpora included in the NLTK, visit http://www.nltk.org/nltk_data/\n",
    "\n",
    "***\n",
    "\n",
    "When you are done, quit the download app, then quit Python by typing **```quit()```**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on importing...\n",
    "\n",
    "What are the differences between the following 2 lines?\n",
    ">```\n",
    "from nltk.tokenize import word_tokenize    \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "```\n",
    "\n",
    "These lines use the following syntax (template) for importing 2 kinds of 'things' into your progam.\n",
    ">```\n",
    "from <module> import <something>\n",
    "```\n",
    "\n",
    "```<module>``` is the name of the module that contains ```<something>``` you are intersted in.\n",
    "\n",
    "When this ```<something>``` is a function, it is usually typed in lower case letters (e.g., word_tokenize). These are tools you can use. It's sort of like you are renting a tool (like a chainsaw). **Usually, tools are good at completing a single task.**\n",
    "\n",
    "When this ```<something>``` is a class, it is usually typed in CamelCase letters. **A class** (such as 'WordPunctTokenizer') is sort of like a company or group of experts. So, when you *call them* using the following syntax:\n",
    "\n",
    ">```\n",
    "tokenizer = WordPunctTokenizer()\n",
    "```\n",
    "\n",
    "you will get to use an expert sent from the company. Typically, these experts (created by **classes**) can do multiple things (unlike functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the beginning God created the heaven and the earth.', 'And the earth was without form, and void; and darkness was upon the face of the deep.', 'And the Spirit of God moved upon the face of the waters.']\n",
      "In the beginning God created the heaven and the earth.\n",
      "And the earth was without form, and void; and darkness was upon the face of the deep.\n",
      "And the Spirit of God moved upon the face of the waters.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Text into Sentences (p.8)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize        # import 'sent_tokenize' function from the 'nltk.tokenize' module\n",
    "\n",
    "s = \"In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\"\n",
    "\n",
    "res = sent_tokenize(s)\n",
    "print(res)\n",
    "for s in res:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences into Words (p.10)\n",
    "\n",
    "from nltk.tokenize import word_tokenize        # import 'sent_tokenize' function from the 'nltk.tokenize' module\n",
    "\n",
    "s = \"In the beginning God created the heaven and the earth.\"\n",
    "res = word_tokenize(s)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'do', \"n't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', ';', 'but', 'that', 'ai', \"n't\", 'no', 'matter', '.']\n",
      "['You', 'don', \"'\", 't', 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', ';', 'but', 'that', 'ain', \"'\", 't', 'no', 'matter', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences into Words (Separating Contractions, p.10)\n",
    "\n",
    "from nltk.tokenize import word_tokenize        # import 'word_tokenize' function from the 'nltk.tokenize' module\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "res = word_tokenize(s)\n",
    "print(res)\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "res = tokenizer.tokenize(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"don't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', 'but', 'that', \"ain't\", 'no', 'matter']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences using Regular Expressions (p.13)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "res = tokenizer.tokenize(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl: But you already have a Big Mac...\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "# Training a Sentence Tokenizer (p.14)\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer  \n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)     # teach the tokenizer\n",
    "\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "print(sents1[678])\n",
    "\n",
    "from nltk.tokenize import sent_tokenize           # default tokenizer\n",
    "sents2 = sent_tokenize(text)\n",
    "print(sents2[678])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"don't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', 'but', 'that', \"ain't\", 'no', 'matter']\n",
      "---\n",
      "['You', \"don't\", 'know', 'without', 'read', 'book', 'name', 'The', 'Adventures', 'Tom', 'Sawyer', \"ain't\", 'matter']\n",
      "['You', \"don't\", 'know', 'without', 'read', 'book', 'name', 'The', 'Adventures', 'Tom', 'Sawyer', \"ain't\", 'matter']\n"
     ]
    }
   ],
   "source": [
    "# Filtering Stopwords in a Tokenized Sentence (p.17)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))    # 'set' is a data type \n",
    "                                                   # A set is an unordered collection with no duplicat elements\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "words = tokenizer.tokenize(s)\n",
    "print(words)\n",
    "print(\"---\")\n",
    "\n",
    "# Let's remove all the stop words.\n",
    "lst = []\n",
    "for word in words:         \n",
    "    if word not in english_stops:\n",
    "        lst.append(word)\n",
    "print(lst)\n",
    "\n",
    "# !!! [word for word in words if word not in english_stops] does almost exactly what the code above does\n",
    "\n",
    "res = [word for word in words if word not in english_stops]   \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('lunch.n.01'), Synset('lunch.v.01'), Synset('lunch.v.02')]\n",
      "---\n",
      "lunch.n.01 â€” a midday meal\n",
      "lunch.v.01 â€” take the midday meal\n",
      "lunch.v.02 â€” provide a midday meal for\n",
      "---\n",
      "food.n.01 â€” any substance that can be metabolized by an animal to give energy and build tissue\n",
      "food.n.02 â€” any solid substance (as opposed to liquid) that is used as a source of nourishment\n",
      "food.n.03 â€” anything that provides mental stimulus for thinking\n"
     ]
    }
   ],
   "source": [
    "# Looking up Synsets for a Word in Wordnet (p.18)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(wordnet.synsets(\"lunch\"))\n",
    "\n",
    "print(\"---\")\n",
    "def word_and_definition(w):\n",
    "    syns = wordnet.synsets(w)              # ask 'wordnet' to return a set of synonyms\n",
    "    for w in syns:                         # for each word in the set of synonyms\n",
    "        print(\"{} â€” {}\".format(w.name(), w.definition()))   # print its name and definition\n",
    "    \n",
    "word_and_definition(\"lunch\")    \n",
    "print(\"---\")\n",
    "word_and_definition(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "fan.n.01 â€” n\n",
      "sports_fan.n.01 â€” n\n",
      "fan.n.03 â€” n\n",
      "fan.v.01 â€” v\n",
      "fan.v.02 â€” v\n",
      "fan.v.03 â€” v\n",
      "winnow.v.01 â€” v\n"
     ]
    }
   ],
   "source": [
    "# Looking up Synsets for a Word in Wordnet (Part of Speech, p20)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(\"---\")\n",
    "def word_and_definition(w):\n",
    "    syns = wordnet.synsets(w)              # ask 'wordnet' to return a set of synonyms\n",
    "    for w in syns:                         # for each word in the set of synonyms\n",
    "        print(\"{} â€” {}\".format(w.name(), w.pos()))   # print its name and part of speech\n",
    "    \n",
    "word_and_definition(\"fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn.name() = lunch.n.01\n",
      "\n",
      "syn.lemmas() = [Lemma('lunch.n.01.lunch'), Lemma('lunch.n.01.luncheon'), Lemma('lunch.n.01.tiffin'), Lemma('lunch.n.01.dejeuner')]\n",
      "\n",
      "syn.lemma_names() = ['lunch', 'luncheon', 'tiffin', 'dejeuner']\n",
      "\n",
      "lunch\n",
      "luncheon\n",
      "tiffin\n",
      "dejeuner\n"
     ]
    }
   ],
   "source": [
    "# Looking up Lemmas and Synonyms in WordNet (p.21)\n",
    "\n",
    "# !!! -- A lemma is a word you find in the dictionary\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"lunch\")[0]                           # first element in the synsets\n",
    "print(\"syn.name() = {}\\n\".format(syn.name()))\n",
    "print(\"syn.lemmas() = {}\\n\".format(syn.lemmas()))             # get synonyms objects\n",
    "print(\"syn.lemma_names() = {}\\n\".format(syn.lemma_names()))   # get synonyms but just names\n",
    "\n",
    "lemmas = syn.lemmas()\n",
    "\n",
    "for w in lemmas:\n",
    "    print(w.name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunch vs. dinner:    0.875\n",
      "lunch vs. frog:      0.211\n",
      "lunch vs. academic:  0.250\n"
     ]
    }
   ],
   "source": [
    "# Calculating WordNet Synset Similarity (p.)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lunch  = wordnet.synsets(\"lunch\")[0]\n",
    "dinner = wordnet.synsets(\"dinner\")[0]\n",
    "frog   = wordnet.synsets(\"frog\")[0]\n",
    "acad   = wordnet.synsets(\"academic\")[0]\n",
    "\n",
    "print(\"lunch vs. dinner:    {:.3f}\".format(lunch.wup_similarity(dinner)))\n",
    "print(\"lunch vs. frog:      {:.3f}\".format(lunch.wup_similarity(frog)))\n",
    "print(\"lunch vs. academic:  {:.3f}\".format(lunch.wup_similarity(acad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', 'be'),\n",
       " ('freedom', 'ring'),\n",
       " ('ring', 'from'),\n",
       " ('one', 'day'),\n",
       " ('a', 'dream')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discovering Word Collocations (p.25)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/mlk_i_have_a_dream_1963.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s)   # tokenize the text\n",
    "# print(words)\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Clinton ----\n",
      "[   ('we', 'will'),\n",
      "    ('donald', 'trump'),\n",
      "    ('each', 'other'),\n",
      "    ('our', 'country'),\n",
      "    ('if', 'you'),\n",
      "    ('tell', 'you'),\n",
      "    ('years', 'ago'),\n",
      "    ('thank', 'you'),\n",
      "    ('police', 'officers'),\n",
      "    ('millions', 'of')]\n",
      "\n",
      "---- Clinton (without stop words) ----\n",
      "[   ('donald', 'trump'),\n",
      "    ('years', 'ago'),\n",
      "    ('police', 'officers'),\n",
      "    ('donald', \"trump's\"),\n",
      "    ('united', 'states'),\n",
      "    ('small', 'businesses'),\n",
      "    ('young', 'people'),\n",
      "    ('years', 'later'),\n",
      "    ('middle', 'class'),\n",
      "    ('sales', 'pitch')]\n",
      "\n",
      "---- Trump ----\n",
      "[   ('going', 'to'),\n",
      "    ('our', 'country'),\n",
      "    ('hillary', 'clinton'),\n",
      "    ('i', 'am'),\n",
      "    ('we', 'will'),\n",
      "    ('will', 'be'),\n",
      "    ('my', 'opponent'),\n",
      "    ('i', 'have'),\n",
      "    ('believe', 'me'),\n",
      "    ('united', 'states')]\n",
      "\n",
      "---- Trump (without stop words) ----\n",
      "[   ('hillary', 'clinton'),\n",
      "    ('united', 'states'),\n",
      "    ('years', 'ago'),\n",
      "    ('president', 'obama'),\n",
      "    ('white', 'house'),\n",
      "    (\"we're\", 'going'),\n",
      "    ('never', 'ever'),\n",
      "    ('last', 'year'),\n",
      "    ('far', 'less'),\n",
      "    ('special', 'interests')]\n"
     ]
    }
   ],
   "source": [
    "# Discovering Word Collocations (p.25)\n",
    "import pprint                                  # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "stopset = set(stopwords.words('english'))    # 'set' is a data type \n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "# fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "fhand.seek(0)\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset     # define a procedure that identifies a stop word\n",
    "bcf.apply_word_filter(filter_stops)                     # ask 'bcf' to apply the filtering procedure\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton (without stop words) ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand = open(\"text/trump_rnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand.seek(0)\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump (without stop words) ----\")\n",
    "pp.pprint(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Clinton ----\n",
      "[   ('donald', 'trump', 'says'),\n",
      "    ('donald', 'trump', 'refused'),\n",
      "    ('wisconsin', 'donald', 'trump'),\n",
      "    ('think', 'donald', 'trump'),\n",
      "    ('donald', 'trump', \"doesn't\"),\n",
      "    ('chief', 'donald', 'trump'),\n",
      "    ('donald', 'trump', \"can't\"),\n",
      "    ('donald', 'trump', 'donald'),\n",
      "    ('trump', 'donald', 'trump'),\n",
      "    ('240', 'years', 'ago')]\n",
      "\n",
      "---- Trump ----\n",
      "[   ('hillary', 'clinton', 'death'),\n",
      "    ('hillary', 'clinton', 'plans'),\n",
      "    ('hillary', 'clinton', 'pushed'),\n",
      "    ('hillary', 'clinton', 'remember'),\n",
      "    ('yet', 'hillary', 'clinton'),\n",
      "    ('put', 'hillary', 'clinton'),\n",
      "    ('hillary', 'clinton', 'americans'),\n",
      "    ('united', 'states', 'gov'),\n",
      "    ('united', 'states', 'supreme'),\n",
      "    ('eight', 'years', 'ago')]\n"
     ]
    }
   ],
   "source": [
    "# TrigramCollocation Finder (p.26)\n",
    "\n",
    "import pprint                                  # pretty printing for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "stopset = set(stopwords.words('english'))    # 'set' is a data type \n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "res = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand = open(\"text/trump_rnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text\n",
    "\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "res = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump ----\")\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
