{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing with NLTK3 Cookbook\n",
    "\n",
    "## Chpater 1 Tokenizing Text and WordNet Basics\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start...\n",
    "\n",
    "Before you start executing the programs on this page, You need to install the Natural Language Tool Kit (NLTK). You will probably need 10-15 minutes for this task. Installing the NLTK is relatively quick. Downloading NTLK corpora may require some time (a comple of minutes each).\n",
    "\n",
    "### Installing the NLTK \n",
    "\n",
    "1. Open Terminal, and change your current folder to the course folder (e.g., ```Desktop/cfh/```)\n",
    "2. Activate the virtual environment\n",
    "3. Type the following line, and press the return key.\n",
    "\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "Wait until it is doen downloading the NLTK succesfully. (If something goes wrong here, pelase contact me.)\n",
    "\n",
    "Once the NLTK has been downloaded successfully, you can download some NLTK corpora.\n",
    "\n",
    "### Downloading NLTK corpora\n",
    "\n",
    "1. Open Terminal, and change your current folder to the course folder (e.g., ```Desktop/cfh/```)\n",
    "2. Activate the virtual environment\n",
    "3. Start Python (by typing 'python' in the command line)\n",
    "4. import nltk by typing: **```import nltk```**\n",
    "5. launch the NLTK download app by typing: **```nltk.download()```**\n",
    "\n",
    "The following example shows Steps 3-5:\n",
    "\n",
    ">```\n",
    "(ven) <your_username>/Desktop/cfh % python\n",
    "Python 3.5.1 |Anaconda 4.0.0 (x86_64)| (default, Dec  7 2015, 11:24:55) \n",
    "[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>>> import nltk\n",
    ">>>> nltk.download()\n",
    "```\n",
    "\n",
    "This will launch a NLTK download app for downloading data.\n",
    "\n",
    "Select the **Corpora** tab, and download the following items:\n",
    "\n",
    "- webtext\n",
    "- wordnet\n",
    "- stopwords\n",
    "\n",
    "When you are done, quit the download app, then quit python (by typing **```quit()```**).\n",
    "\n",
    "**CAUTION !!** \u001f-— Don't double click these items. It will start downloading. In particular, DON'T double click \"all\", \"all-corpus\", or \"book\" under the Collections tab. Clicking those items will start downloading MANY things, and it will take a LONG time.\n",
    "\n",
    "Reference: http://www.nltk.org/data.html\n",
    "\n",
    "***\n",
    "\n",
    "If you decide to download everything, do it when you are not using your computer for a while. It will take a long time.\n",
    "\n",
    "If you are interested in the complete listing of corpora included in the NLTK, visit the followin page.\n",
    "\n",
    "http://www.nltk.org/nltk_data/\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on importing...\n",
    "\n",
    "What are the differences between the following 2 lines?\n",
    ">```\n",
    "from nltk.tokenize import word_tokenize    \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "```\n",
    "\n",
    "These lines use the following syntax (template) for importing 2 kinds of 'things' into your progam.\n",
    ">```\n",
    "from <module> import <something>\n",
    "```\n",
    "\n",
    "```<module>``` is the name of the module that contains ```<something>``` you are intersted in.\n",
    "\n",
    "When this ```<something>``` is a function, it is usually typed in lower case letters (e.g., word_tokenize). These are tools you can use. It's sort of like you are renting a tool (like a chainsaw). **Usually, tools are good at completing a single task.**\n",
    "\n",
    "When this ```<something>``` is a class, it is usually typed in CamelCase letters. **A class** (such as 'WordPunctTokenizer) is sort of like a group of expert, or a company. So, when you *call them* using the following syntax:\n",
    "\n",
    ">```\n",
    "tokenizer = WordPunctTokenizer()\n",
    "```\n",
    "\n",
    "you will get to use an expert sent from the company. Typically, these experts (created by **classes**) can do multiple things (unlike functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the beginning God created the heaven and the earth.', 'And the earth was without form, and void; and darkness was upon the face of the deep.', 'And the Spirit of God moved upon the face of the waters.']\n",
      "In the beginning God created the heaven and the earth.\n",
      "And the earth was without form, and void; and darkness was upon the face of the deep.\n",
      "And the Spirit of God moved upon the face of the waters.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Text into Sentences (p.8)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize        # import 'sent_tokenise' function from the 'nltk.tokeniz' module\n",
    "\n",
    "s = \"In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\"\n",
    "\n",
    "res = sent_tokenize(s)\n",
    "print(res)\n",
    "for s in res:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences into Words (p.10)\n",
    "\n",
    "from nltk.tokenize import word_tokenize        # import 'sent_tokenise' function from the 'nltk.tokeniz' module\n",
    "\n",
    "s = \"In the beginning God created the heaven and the earth.\"\n",
    "res = word_tokenize(s)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'do', \"n't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', ';', 'but', 'that', 'ai', \"n't\", 'no', 'matter', '.']\n",
      "['You', 'don', \"'\", 't', 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', ';', 'but', 'that', 'ain', \"'\", 't', 'no', 'matter', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences into Words (Separating Contractions, p.10)\n",
    "\n",
    "from nltk.tokenize import word_tokenize        # import 'word_tokenise' function from the 'nltk.tokeniz' module\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "res = word_tokenize(s)\n",
    "print(res)\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "res = tokenizer.tokenize(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"don't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', 'but', 'that', \"ain't\", 'no', 'matter']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Sentences using Regular Expressions (p.13)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "res = tokenizer.tokenize(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl: But you already have a Big Mac...\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "# Training a Sentence Tokenizer (p.14)\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer  \n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)     # teach the tokenizer\n",
    "\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "print(sents1[678])\n",
    "\n",
    "from nltk.tokenize import sent_tokenize           # default tokenizer\n",
    "sents2 = sent_tokenize(text)\n",
    "print(sents2[678])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"don't\", 'know', 'about', 'me', 'without', 'you', 'have', 'read', 'a', 'book', 'by', 'the', 'name', 'of', 'The', 'Adventures', 'of', 'Tom', 'Sawyer', 'but', 'that', \"ain't\", 'no', 'matter']\n",
      "---\n",
      "['You', \"don't\", 'know', 'without', 'read', 'book', 'name', 'The', 'Adventures', 'Tom', 'Sawyer', \"ain't\", 'matter']\n",
      "['You', \"don't\", 'know', 'without', 'read', 'book', 'name', 'The', 'Adventures', 'Tom', 'Sawyer', \"ain't\", 'matter']\n"
     ]
    }
   ],
   "source": [
    "# Filtering Stopwords in a Tokenized Sentence (p.17)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))    # set is a data type. \n",
    "                                                   # A set is an unordered collection with no duplicat elements.\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "s = \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter.\"\n",
    "words = tokenizer.tokenize(s)\n",
    "print(words)\n",
    "print(\"---\")\n",
    "\n",
    "# Let's remove all the stop words.\n",
    "lst = []\n",
    "for word in words:         \n",
    "    if word not in english_stops:\n",
    "        lst.append(word)\n",
    "print(lst)\n",
    "\n",
    "# !!! [word for word in words if word not in english_stops] basically does what the above code does.\n",
    "\n",
    "res = [word for word in words if word not in english_stops]   \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('lunch.n.01'), Synset('lunch.v.01'), Synset('lunch.v.02')]\n",
      "---\n",
      "lunch.n.01 — a midday meal\n",
      "lunch.v.01 — take the midday meal\n",
      "lunch.v.02 — provide a midday meal for\n",
      "---\n",
      "food.n.01 — any substance that can be metabolized by an animal to give energy and build tissue\n",
      "food.n.02 — any solid substance (as opposed to liquid) that is used as a source of nourishment\n",
      "food.n.03 — anything that provides mental stimulus for thinking\n"
     ]
    }
   ],
   "source": [
    "# Looking up Synsets for a Word in Wordnet (p.18)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(wordnet.synsets(\"lunch\"))\n",
    "\n",
    "print(\"---\")\n",
    "def word_and_definition(w):\n",
    "    syns = wordnet.synsets(w)              # ask 'wordnet' to return a set of synonyms.\n",
    "    for w in syns:                         # for each word in the set of synonyms,\n",
    "        print(\"{} — {}\".format(w.name(), w.definition()))   # print its name and definition\n",
    "    \n",
    "word_and_definition(\"lunch\")    \n",
    "print(\"---\")\n",
    "word_and_definition(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "fan.n.01 — n\n",
      "sports_fan.n.01 — n\n",
      "fan.n.03 — n\n",
      "fan.v.01 — v\n",
      "fan.v.02 — v\n",
      "fan.v.03 — v\n",
      "winnow.v.01 — v\n"
     ]
    }
   ],
   "source": [
    "# Looking up Synsets for a Word in Wordnet (Part of Speech, p20)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(\"---\")\n",
    "def word_and_definition(w):\n",
    "    syns = wordnet.synsets(w)              # ask 'wordnet' to return a set of synonyms.\n",
    "    for w in syns:                         # for each word in the set of synonyms,\n",
    "        print(\"{} — {}\".format(w.name(), w.pos()))   # print its name and part of speech\n",
    "    \n",
    "word_and_definition(\"fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn.name() = lunch.n.01\n",
      "\n",
      "syn.lemmas() = [Lemma('lunch.n.01.lunch'), Lemma('lunch.n.01.luncheon'), Lemma('lunch.n.01.tiffin'), Lemma('lunch.n.01.dejeuner')]\n",
      "\n",
      "syn.lemma_names() = ['lunch', 'luncheon', 'tiffin', 'dejeuner']\n",
      "\n",
      "lunch\n",
      "luncheon\n",
      "tiffin\n",
      "dejeuner\n"
     ]
    }
   ],
   "source": [
    "# Looking up Lemmas and Synonyms in WordNet (p.21)\n",
    "\n",
    "# !!! -- A lemma is the word you find in the dictionary.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"lunch\")[0]                           # first element in the synsets\n",
    "print(\"syn.name() = {}\\n\".format(syn.name()))\n",
    "print(\"syn.lemmas() = {}\\n\".format(syn.lemmas()))             # get synonyms objects\n",
    "print(\"syn.lemma_names() = {}\\n\".format(syn.lemma_names()))   # get synonyms but just names.\n",
    "\n",
    "lemmas = syn.lemmas()\n",
    "\n",
    "for w in lemmas:\n",
    "    print(w.name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lunch vs. dinner:    0.875\n",
      "lunch vs. frog:      0.211\n",
      "lunch vs. academic:  0.250\n"
     ]
    }
   ],
   "source": [
    "# Calculating WordNet Synset Similarity (p.)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lunch  = wordnet.synsets(\"lunch\")[0]\n",
    "dinner = wordnet.synsets(\"dinner\")[0]\n",
    "frog   = wordnet.synsets(\"frog\")[0]\n",
    "acad   = wordnet.synsets(\"academic\")[0]\n",
    "\n",
    "print(\"lunch vs. dinner:    {:.3f}\".format(lunch.wup_similarity(dinner)))\n",
    "print(\"lunch vs. frog:      {:.3f}\".format(lunch.wup_similarity(frog)))\n",
    "print(\"lunch vs. academic:  {:.3f}\".format(lunch.wup_similarity(acad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', 'be'),\n",
       " ('freedom', 'ring'),\n",
       " ('ring', 'from'),\n",
       " ('one', 'day'),\n",
       " ('a', 'dream')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discovering Word Collocations (p.25)\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/mlk_i_have_a_dream_1963.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s)   # tokenize the text.\n",
    "# print(words)\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Clinton ----\n",
      "[   ('we', 'will'),\n",
      "    ('donald', 'trump'),\n",
      "    ('each', 'other'),\n",
      "    ('our', 'country'),\n",
      "    ('if', 'you'),\n",
      "    ('tell', 'you'),\n",
      "    ('years', 'ago'),\n",
      "    ('thank', 'you'),\n",
      "    ('police', 'officers'),\n",
      "    ('millions', 'of')]\n",
      "\n",
      "---- Clinton (without stop words) ----\n",
      "[   ('donald', 'trump'),\n",
      "    ('years', 'ago'),\n",
      "    ('police', 'officers'),\n",
      "    ('donald', \"trump's\"),\n",
      "    ('united', 'states'),\n",
      "    ('small', 'businesses'),\n",
      "    ('young', 'people'),\n",
      "    ('years', 'later'),\n",
      "    ('middle', 'class'),\n",
      "    ('sales', 'pitch')]\n",
      "\n",
      "---- Trump ----\n",
      "[   ('going', 'to'),\n",
      "    ('our', 'country'),\n",
      "    ('hillary', 'clinton'),\n",
      "    ('i', 'am'),\n",
      "    ('we', 'will'),\n",
      "    ('will', 'be'),\n",
      "    ('my', 'opponent'),\n",
      "    ('i', 'have'),\n",
      "    ('believe', 'me'),\n",
      "    ('united', 'states')]\n",
      "\n",
      "---- Trump (without stop words) ----\n",
      "[   ('hillary', 'clinton'),\n",
      "    ('united', 'states'),\n",
      "    ('years', 'ago'),\n",
      "    ('president', 'obama'),\n",
      "    ('white', 'house'),\n",
      "    (\"we're\", 'going'),\n",
      "    ('never', 'ever'),\n",
      "    ('last', 'year'),\n",
      "    ('far', 'less'),\n",
      "    ('special', 'interests')]\n"
     ]
    }
   ],
   "source": [
    "# Discovering Word Collocations (p.25)\n",
    "import pprint                                  # pretty prnting for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "stopset = set(stopwords.words('english'))    # set is a data type. \n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "# fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "fhand.seek(0)\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset     # defins a procedure that identifies a stop word\n",
    "bcf.apply_word_filter(filter_stops)                     # ask 'bcf' to apply the filtering procedure\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton (without stop words) ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand = open(\"text/trump_rnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand.seek(0)\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "res = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump (without stop words) ----\")\n",
    "pp.pprint(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Clinton ----\n",
      "[   ('donald', 'trump', 'says'),\n",
      "    ('donald', 'trump', 'refused'),\n",
      "    ('wisconsin', 'donald', 'trump'),\n",
      "    ('think', 'donald', 'trump'),\n",
      "    ('donald', 'trump', \"doesn't\"),\n",
      "    ('chief', 'donald', 'trump'),\n",
      "    ('donald', 'trump', \"can't\"),\n",
      "    ('donald', 'trump', 'donald'),\n",
      "    ('trump', 'donald', 'trump'),\n",
      "    ('240', 'years', 'ago')]\n",
      "\n",
      "---- Trump ----\n",
      "[   ('hillary', 'clinton', 'death'),\n",
      "    ('hillary', 'clinton', 'plans'),\n",
      "    ('hillary', 'clinton', 'pushed'),\n",
      "    ('hillary', 'clinton', 'remember'),\n",
      "    ('yet', 'hillary', 'clinton'),\n",
      "    ('put', 'hillary', 'clinton'),\n",
      "    ('hillary', 'clinton', 'americans'),\n",
      "    ('united', 'states', 'gov'),\n",
      "    ('united', 'states', 'supreme'),\n",
      "    ('eight', 'years', 'ago')]\n"
     ]
    }
   ],
   "source": [
    "# .p26 TrigramCollocation Finder\n",
    "\n",
    "import pprint                                  # pretty prnting for debugging\n",
    "pp = pprint.PrettyPrinter(indent=4)            # create a pretty printing object used for debugging.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer      \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "stopset = set(stopwords.words('english'))    # set is a data type. \n",
    "tokenizer = RegexpTokenizer(\"['\\w']+\")\n",
    "\n",
    "fhand = open(\"text/clinton_dnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "res = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Clinton ----\")\n",
    "pp.pprint(res)\n",
    "\n",
    "print(\"\")\n",
    "fhand = open(\"text/trump_rnc_speech_2016.txt\")\n",
    "s = fhand.read()\n",
    "words = tokenizer.tokenize(s.lower())   # tokenize the text.\n",
    "\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "res = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"---- Trump ----\")\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
