{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with TextBlob\n",
    "\n",
    "**Execute the following 2 lines in the Terminal Window.** before using TextBlob\n",
    "\n",
    "> ```\n",
    "pip install -U textblob\n",
    "```\n",
    "```\n",
    "python -m textblob.download_corpora\n",
    "```\n",
    "\n",
    "Reference: https://textblob.readthedocs.io/en/dev/index.html See under \"Get it Now\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poloarity = 0.06597222222222222\n",
      "subjectivity = 0.38611111111111107\n",
      "---\n",
      "poloarity = 0.13333333333333333\n",
      "subjectivity = 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.13333333333333333, subjectivity=0.6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis\n",
    "# The following is an example of doing a sentiment analysis (Using the 'PatternAnalyzer' = default)\n",
    "# See the next code cell for another method (which is probably more useful)\n",
    "\n",
    "from textblob import TextBlob\n",
    "txt1 = \"\"\"Save yourself the let down and buy yourself a different fitness tracker, unless you're \n",
    "into dissapointment then this for you. Was constantly going into sleep mode and not waking up, battery \n",
    "didn't last as long as claimed, and to top it all off after 3 weeks of mild use it just stopped working. \n",
    "Never got wet or misused, but for some reason no longer counts steps.\"\"\"\n",
    "\n",
    "txt2 =\"\"\"\\\"We are very appreciative of what the Confucius Institute does, which is to build a \n",
    "bridge between Houston and China,\\\" Li said. \\\"We are very thankful for Al Green, who graduated \n",
    "right here from Texas Southern University. I am very surprised so many young children can sing \n",
    "beautiful Chinese songs. More and more American people are learning Chinese and Chinese culture.\\\"\n",
    "\"\"\"\n",
    "\n",
    "tb1 = TextBlob(txt1)\n",
    "print(\"polarity = {}\".format(tb1.sentiment.polarity))\n",
    "print(\"subjectivity = {}\".format(tb1.sentiment.subjectivity))\n",
    "print(\"---\")\n",
    "tb2 = TextBlob(txt3)\n",
    "print(\"polarity = {}\".format(tb2.sentiment.polarity))\n",
    "print(\"subjectivity = {}\".format(tb2.sentiment.subjectivity))\n",
    "tb2.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='neg', p_pos=0.34288738653147416, p_neg=0.6571126134685206)\n",
      "classification = neg\n",
      "p_pos = 0.34288738653147416\n",
      "p_neg = 0.6571126134685206\n",
      "---\n",
      "Sentiment(classification='pos', p_pos=0.9995404227787248, p_neg=0.00045957722127985275)\n",
      "classification = pos\n",
      "p_pos = 0.9995404227787248\n",
      "p_neg = 0.00045957722127985275\n",
      "[Sentence(\"\"We are very appreciative of what the Confucius Institute does, which is to build a \n",
      "bridge between Houston and China,\" Li said.\"), Sentence(\"\"We are very thankful for Al Green, who graduated \n",
      "right here from Texas Southern University.\"), Sentence(\"I am very surprised so many young children can sing \n",
      "beautiful Chinese songs.\"), Sentence(\"More and more American people are learning Chinese and Chinese culture.\"\")]\n"
     ]
    }
   ],
   "source": [
    "# Refernece: https://textblob.readthedocs.io/en/dev/advanced_usage.html#pos-taggers\n",
    "# The following is an example of doing a sentiment analysis (Using the 'NaiveBayesAnalyzer' = default)\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "txt1 = \"\"\"Save yourself the let down and buy yourself a different fitness tracker, unless you're \n",
    "into dissapointment then this for you. Was constantly going into sleep mode and not waking up, battery \n",
    "didn't last as long as claimed, and to top it all off after 3 weeks of mild use it just stopped working. \n",
    "Never got wet or misused, but for some reason no longer counts steps.\"\"\"\n",
    "\n",
    "txt2 =\"\"\"\\\"We are very appreciative of what the Confucius Institute does, which is to build a \n",
    "bridge between Houston and China,\\\" Li said. \\\"We are very thankful for Al Green, who graduated \n",
    "right here from Texas Southern University. I am very surprised so many young children can sing \n",
    "beautiful Chinese songs. More and more American people are learning Chinese and Chinese culture.\\\"\n",
    "\"\"\"\n",
    "tb1 = TextBlob(txt1, analyzer=NaiveBayesAnalyzer())                 # analyze 'txt1'\n",
    "print(tb1.sentiment)                                                # print the sentiment objct\n",
    "print(\"classification = {}\".format(tb1.sentiment.classification))   # get the classification\n",
    "print(\"p_pos = {}\".format(tb1.sentiment.p_pos))                     # get the positive-ness value\n",
    "print(\"p_neg = {}\".format(tb1.sentiment.p_neg))                     # get the negativeness value\n",
    "\n",
    "print(\"---\")\n",
    "tb2 = TextBlob(txt2, analyzer=NaiveBayesAnalyzer())                 # analyze 'txt2'\n",
    "print(tb2.sentiment)\n",
    "print(\"classification = {}\".format(tb2.sentiment.classification))\n",
    "print(\"p_pos = {}\".format(tb2.sentiment.p_pos))\n",
    "print(\"p_neg = {}\".format(tb2.sentiment.p_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save yourself the let down and buy yourself a different fitness tracker, unless you're  into dissapointment then this for you.\n",
      "---\n",
      "Was constantly going into sleep mode and not waking up, battery  didn't last as long as claimed, and to top it all off after 3 weeks of mild use it just stopped working.\n",
      "---\n",
      "Never got wet or misused, but for some reason no longer counts steps.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# TextBlob can be used to separate a text into sentences.\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "txt1 = \"\"\"Save yourself the let down and buy yourself a different fitness tracker, unless you're \n",
    "into dissapointment then this for you. Was constantly going into sleep mode and not waking up, battery \n",
    "didn't last as long as claimed, and to top it all off after 3 weeks of mild use it just stopped working. \n",
    "Never got wet or misused, but for some reason no longer counts steps.\"\"\"\n",
    "\n",
    "tb1 = TextBlob(txt1)\n",
    "for s in tb1.sentences:\n",
    "    print(s.strip().replace(\"\\n\", \" \"))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emale', 'JJ'), ('suicide', 'NN'), ('bombers', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('Caucasus', 'NNP'), ('known', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('Black', 'NNP'), ('Widows', 'NNP'), ('have', 'VBP'), ('targeted', 'VBN'), ('Russian', 'JJ'), ('civilians', 'NNS'), ('and', 'CC'), ('security', 'NN'), ('personnel', 'NNS'), ('in', 'IN'), ('multiple', 'JJ'), ('attacks', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('decade', 'NN')]\n",
      "---\n",
      "['emale suicide bombers', 'caucasus', 'black widows', 'russian civilians', 'security personnel', 'multiple attacks', 'past decade']\n"
     ]
    }
   ],
   "source": [
    "# A couple of other things TextBlob may be useful\n",
    "\n",
    "s = \"\"\"emale suicide bombers from the Caucasus, known as the Black Widows, have targeted Russian \n",
    "civilians and security personnel in multiple attacks over the past decade\"\"\"\n",
    "\n",
    "tb = TextBlob(s)\n",
    "\n",
    "print(tb.tags)            # Part of Speech Tags\n",
    "print(\"---\")\n",
    "\n",
    "print(tb.noun_phrases)    # List of noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fire', 'NNP'), ('camp', 'NN'), ('crews', 'NNS'), ('provide', 'VBP'), ('huge', 'JJ'), ('support', 'NN'), ('service', 'NN'), ('for', 'IN'), ('Alaska', 'NNP'), ('firefighters', 'NNS')]\n",
      "YES\n",
      "---\n",
      "[('We', 'PRP'), ('support', 'VBP'), ('both', 'DT'), ('Windows', 'NNP'), ('and', 'CC'), ('Mac', 'NNP'), ('systems', 'NNS')]\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "# Find a specific word with a specific POS.\n",
    "\n",
    "# Reference for TAGS: https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "\n",
    "s1 = \"Fire camp crews provide huge support service for Alaska firefighters.\"\n",
    "s2 = \"We support both Windows and Mac systems.\"\n",
    "\n",
    "tb1 = TextBlob(s1)\n",
    "print(tb1.tags)                         # Part of Speech Tags\n",
    "if ('support', \"NN\") in tb1.tags:\n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(\"NO\")    \n",
    "\n",
    "print(\"---\")    \n",
    "\n",
    "tb2 = TextBlob(s2)\n",
    "print(tb2.tags)                         # Part of Speech Tags\n",
    "if ('support', \"NN\") in tb2.tags:       # \n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(\"NO\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire NNP\n",
      "camp NN\n",
      "crews NNS\n",
      "provide VBP\n",
      "   **** Verb, non-3rd person singular present\n",
      "huge JJ\n",
      "support NN\n",
      "service NN\n",
      "for IN\n",
      "Alaska NNP\n",
      "firefighters NNS\n"
     ]
    }
   ],
   "source": [
    "# Find a specific POS in a sentence.\n",
    "\n",
    "# Reference for TAGS: https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "\n",
    "s1 = \"Fire camp crews provide huge support service for Alaska firefighters.\"\n",
    "\n",
    "tb1 = TextBlob(s1)\n",
    "for key, value in tb1.tags:     # for each key and value pair in the list.\n",
    "    print(key, value)           # print 'key' and 'value'\n",
    "    if value == \"VBP\":          # if 'value' is a verb (non-3rd peson singluar present), print.\n",
    "        print(\"   **** Verb, non-3rd person singular present\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
